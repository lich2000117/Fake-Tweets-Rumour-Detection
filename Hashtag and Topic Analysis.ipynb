{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c753f6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ID_PATH   = \"./task2/data/covid.data.txt\"\n",
    "DATA_PATH = \"./task2/data/full_covid.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4231d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_replies_id_to_sorted_text(list_of_id, data_df):\n",
    "    \"\"\"Convert a list of tweet ids to a list of its corresponding text in data_df\n",
    "    Sort by tweets original create time\"\"\"\n",
    "    if type(list_of_id) is not list:\n",
    "        # if input is a single id convert type\n",
    "        list_of_id = [list_of_id]\n",
    "    # Select dataframe based on a list of ids\n",
    "    selected_df = data_df[data_df['id'].astype(str).isin(list_of_id)]\n",
    "    # Sort by their created time\n",
    "    sorted_df = selected_df.sort_values(by = [\"created_at\"])\n",
    "    # Select text fields only\n",
    "    text_list = sorted_df[\"text\"].to_list()\n",
    "    return text_list\n",
    "\n",
    "\n",
    "def remove_URL(original):\n",
    "    \"\"\"Remove url link in the text\"\"\"\n",
    "    result = re.sub(r\"http\\S+\", \"\", original)\n",
    "    result = re.sub(r\"www.\\S+\", \"\", result)\n",
    "    result = re.sub(r\"wasap.my+\", \"\", result)\n",
    "    return result\n",
    "\n",
    "def join_data_id_label_v2(LABEL_PATH, ID_PATH, DATA_PATH):\n",
    "    # Process Labels\n",
    "    if LABEL_PATH:\n",
    "        # 1: Rumour\n",
    "        # 0: NonRumour\n",
    "        with open(LABEL_PATH, \"r\") as f:\n",
    "            y_label = f.read().strip().split(\"\\n\") # remove next line\n",
    "        y_label = pd.DataFrame(y_label, columns = [\"label\"])\n",
    "        y_label[y_label[\"label\"]==\"rumour\"] = 1\n",
    "        y_label[y_label[\"label\"]==\"nonrumour\"] = 0\n",
    "\n",
    "    ## Get Dataframe Id, with first id as source Id, and values as replies, not using dict since we have duplicated keys\n",
    "    total_id_list = []\n",
    "    with open(ID_PATH, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(',') # remove next line\n",
    "            source_id = line[0]\n",
    "            if len(line) > 1:\n",
    "                # if we have replies id\n",
    "                replies_id = line[1:]\n",
    "            else:\n",
    "                replies_id = []\n",
    "            row = [source_id, replies_id]\n",
    "            total_id_list.append(row)\n",
    "    len(total_id_list)\n",
    "    \n",
    "    ## Create a dataframe containing a list of replies\n",
    "    source_df = pd.DataFrame(total_id_list, columns = ['source_id', 'replies_id'])\n",
    "    data_df = pd.read_json(DATA_PATH)\n",
    "    source_df[\"reply_text_list\"] = source_df[\"replies_id\"].apply(convert_replies_id_to_sorted_text, data_df = data_df)\n",
    "    source_df[\"source_text\"] = source_df[\"source_id\"].apply(convert_replies_id_to_sorted_text, data_df = data_df)\n",
    "\n",
    "    if LABEL_PATH:\n",
    "      source_df[\"label\"] = y_label\n",
    "    return source_df\n",
    "\n",
    "def preprocess(IDS, DATA, LABELS=False):\n",
    "    \"\"\"Function to combine all the preprocessing steps\"\"\"\n",
    "    data = join_data_id_label_v2(LABELS, IDS, DATA)\n",
    "    ## 1. Only keep english tweets as most of them are in english\n",
    "    # Use only text data and remove URLs\n",
    "    #data[\"source\"] = data[\"source_text\"].apply(\" \".join).apply(p.clean) \n",
    "    #data[\"replies\"] = data[\"reply_text_list\"].apply(\" \".join).apply(p.clean)\n",
    "    data[\"source\"] = data[\"source_text\"].apply(\" \".join)\n",
    "    data[\"replies\"] = data[\"reply_text_list\"].apply(\" \".join)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "656151c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove Stopwords\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def text_processing(input_str):\n",
    "    \"\"\"A function takes a input text string and output a list of preprocessed words\n",
    "        1. Remove URL\n",
    "        2. StopWords Removal\n",
    "        3. Remove Non-English Words\n",
    "        4. Remove Numeric Value\n",
    "        5. Lemmatize\n",
    "    \n",
    "    \"\"\"\n",
    "    list_words = []\n",
    "    # Remove URL\n",
    "    input_str = Remove_URL(input_str)\n",
    "    for word in re.sub(r'[^\\w\\s]','', input_str).split():\n",
    "        # Remove Numeric Value, Non-English words and Stop words\n",
    "        if (word.isalpha()) and (isEnglish(word)) and (word not in stopwords.words('english')):\n",
    "            # Remove Links in the text\n",
    "            # Lemmatize words\n",
    "            word = lemmatizer.lemmatize(word)\n",
    "            # Add to List\n",
    "            list_words.append(word)\n",
    "    return list_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83faac70",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocess(ID_PATH, DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9adb34b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_id</th>\n",
       "      <th>replies_id</th>\n",
       "      <th>reply_text_list</th>\n",
       "      <th>source_text</th>\n",
       "      <th>source</th>\n",
       "      <th>replies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1272262651100434433</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[According to the New York Times, Warner Bros....</td>\n",
       "      <td>According to the New York Times, Warner Bros. ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1287153210990395392</td>\n",
       "      <td>[1287191952115605505]</td>\n",
       "      <td>[@TexasTribune Guess what the cause of death i...</td>\n",
       "      <td>[Hurricane Hanna has made landfall in Texas.\\n...</td>\n",
       "      <td>Hurricane Hanna has made landfall in Texas.\\n\\...</td>\n",
       "      <td>@TexasTribune Guess what the cause of death is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1266555444283179008</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Monkeys on the loose in India with stolen cor...</td>\n",
       "      <td>Monkeys on the loose in India with stolen coro...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1257715199655755779</td>\n",
       "      <td>[1258212704961155073, 1257843417503141895, 125...</td>\n",
       "      <td>[@BelAkinyii When was the last time you washed...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>@BelAkinyii When was the last time you washed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1274338812173393920</td>\n",
       "      <td>[1274369294558801921, 1274413352828186624, 127...</td>\n",
       "      <td>[@HeidiNBC These Trump fans have a right to at...</td>\n",
       "      <td>[“If Trump felt comfortable having it here, th...</td>\n",
       "      <td>“If Trump felt comfortable having it here, the...</td>\n",
       "      <td>@HeidiNBC These Trump fans have a right to att...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             source_id                                         replies_id  \\\n",
       "0  1272262651100434433                                                 []   \n",
       "1  1287153210990395392                              [1287191952115605505]   \n",
       "2  1266555444283179008                                                 []   \n",
       "3  1257715199655755779  [1258212704961155073, 1257843417503141895, 125...   \n",
       "4  1274338812173393920  [1274369294558801921, 1274413352828186624, 127...   \n",
       "\n",
       "                                     reply_text_list  \\\n",
       "0                                                 []   \n",
       "1  [@TexasTribune Guess what the cause of death i...   \n",
       "2                                                 []   \n",
       "3  [@BelAkinyii When was the last time you washed...   \n",
       "4  [@HeidiNBC These Trump fans have a right to at...   \n",
       "\n",
       "                                         source_text  \\\n",
       "0  [According to the New York Times, Warner Bros....   \n",
       "1  [Hurricane Hanna has made landfall in Texas.\\n...   \n",
       "2  [Monkeys on the loose in India with stolen cor...   \n",
       "3                                                 []   \n",
       "4  [“If Trump felt comfortable having it here, th...   \n",
       "\n",
       "                                              source  \\\n",
       "0  According to the New York Times, Warner Bros. ...   \n",
       "1  Hurricane Hanna has made landfall in Texas.\\n\\...   \n",
       "2  Monkeys on the loose in India with stolen coro...   \n",
       "3                                                      \n",
       "4  “If Trump felt comfortable having it here, the...   \n",
       "\n",
       "                                             replies  \n",
       "0                                                     \n",
       "1  @TexasTribune Guess what the cause of death is...  \n",
       "2                                                     \n",
       "3  @BelAkinyii When was the last time you washed ...  \n",
       "4  @HeidiNBC These Trump fans have a right to att...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9a3c6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_json(path_or_buf=\"./task2/data/processed_covid.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6cc069d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_id</th>\n",
       "      <th>replies_id</th>\n",
       "      <th>reply_text_list</th>\n",
       "      <th>source_text</th>\n",
       "      <th>source</th>\n",
       "      <th>replies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1272262651100434432</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[According to the New York Times, Warner Bros....</td>\n",
       "      <td>According to the New York Times, Warner Bros. ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1287153210990395392</td>\n",
       "      <td>[1287191952115605505]</td>\n",
       "      <td>[@TexasTribune Guess what the cause of death i...</td>\n",
       "      <td>[Hurricane Hanna has made landfall in Texas.\\n...</td>\n",
       "      <td>Hurricane Hanna has made landfall in Texas.\\n\\...</td>\n",
       "      <td>@TexasTribune Guess what the cause of death is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1266555444283179008</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Monkeys on the loose in India with stolen cor...</td>\n",
       "      <td>Monkeys on the loose in India with stolen coro...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1257715199655755776</td>\n",
       "      <td>[1258212704961155073, 1257843417503141895, 125...</td>\n",
       "      <td>[@BelAkinyii When was the last time you washed...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>@BelAkinyii When was the last time you washed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1274338812173393920</td>\n",
       "      <td>[1274369294558801921, 1274413352828186624, 127...</td>\n",
       "      <td>[@HeidiNBC These Trump fans have a right to at...</td>\n",
       "      <td>[“If Trump felt comfortable having it here, th...</td>\n",
       "      <td>“If Trump felt comfortable having it here, the...</td>\n",
       "      <td>@HeidiNBC These Trump fans have a right to att...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17453</th>\n",
       "      <td>1249502859185590272</td>\n",
       "      <td>[1249578608563126272, 1249537088128573441, 124...</td>\n",
       "      <td>[@funder Wonder how many lives could have been...</td>\n",
       "      <td>[I wonder how many lives could’ve been saved i...</td>\n",
       "      <td>I wonder how many lives could’ve been saved if...</td>\n",
       "      <td>@funder Wonder how many lives could have been ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17454</th>\n",
       "      <td>1284050414619459584</td>\n",
       "      <td>[1284173350751928320, 1284415121192898560, 128...</td>\n",
       "      <td>[@NadineDorries @thetimes Inadequate supplies ...</td>\n",
       "      <td>[The @thetimes front page on 17th March. The f...</td>\n",
       "      <td>The @thetimes front page on 17th March. The fi...</td>\n",
       "      <td>@NadineDorries @thetimes Inadequate supplies o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17455</th>\n",
       "      <td>1274505289614725120</td>\n",
       "      <td>[1274548079648223243]</td>\n",
       "      <td>[@DNCWarRoom Fact check: Chinese is not a race...</td>\n",
       "      <td>[Trump just completed the racism trifecta in a...</td>\n",
       "      <td>Trump just completed the racism trifecta in a ...</td>\n",
       "      <td>@DNCWarRoom Fact check: Chinese is not a race....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17456</th>\n",
       "      <td>1267884642637676544</td>\n",
       "      <td>[1267892868603092994]</td>\n",
       "      <td>[@Jess__Taylor__ @davidallengreen Eck! What ar...</td>\n",
       "      <td>[Here are a few of my photographs from today’s...</td>\n",
       "      <td>Here are a few of my photographs from today’s ...</td>\n",
       "      <td>@Jess__Taylor__ @davidallengreen Eck! What are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17457</th>\n",
       "      <td>1265801718958301184</td>\n",
       "      <td>[1265805989888036866, 1265810575231201280, 126...</td>\n",
       "      <td>[@seanhannity DeBlasio caused NYC's lockdown c...</td>\n",
       "      <td>[‘IT’S GONE’: Bill De Blasio Says NYC Facing $...</td>\n",
       "      <td>‘IT’S GONE’: Bill De Blasio Says NYC Facing $9...</td>\n",
       "      <td>@seanhannity DeBlasio caused NYC's lockdown ch...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17458 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 source_id                                         replies_id  \\\n",
       "0      1272262651100434432                                                 []   \n",
       "1      1287153210990395392                              [1287191952115605505]   \n",
       "2      1266555444283179008                                                 []   \n",
       "3      1257715199655755776  [1258212704961155073, 1257843417503141895, 125...   \n",
       "4      1274338812173393920  [1274369294558801921, 1274413352828186624, 127...   \n",
       "...                    ...                                                ...   \n",
       "17453  1249502859185590272  [1249578608563126272, 1249537088128573441, 124...   \n",
       "17454  1284050414619459584  [1284173350751928320, 1284415121192898560, 128...   \n",
       "17455  1274505289614725120                              [1274548079648223243]   \n",
       "17456  1267884642637676544                              [1267892868603092994]   \n",
       "17457  1265801718958301184  [1265805989888036866, 1265810575231201280, 126...   \n",
       "\n",
       "                                         reply_text_list  \\\n",
       "0                                                     []   \n",
       "1      [@TexasTribune Guess what the cause of death i...   \n",
       "2                                                     []   \n",
       "3      [@BelAkinyii When was the last time you washed...   \n",
       "4      [@HeidiNBC These Trump fans have a right to at...   \n",
       "...                                                  ...   \n",
       "17453  [@funder Wonder how many lives could have been...   \n",
       "17454  [@NadineDorries @thetimes Inadequate supplies ...   \n",
       "17455  [@DNCWarRoom Fact check: Chinese is not a race...   \n",
       "17456  [@Jess__Taylor__ @davidallengreen Eck! What ar...   \n",
       "17457  [@seanhannity DeBlasio caused NYC's lockdown c...   \n",
       "\n",
       "                                             source_text  \\\n",
       "0      [According to the New York Times, Warner Bros....   \n",
       "1      [Hurricane Hanna has made landfall in Texas.\\n...   \n",
       "2      [Monkeys on the loose in India with stolen cor...   \n",
       "3                                                     []   \n",
       "4      [“If Trump felt comfortable having it here, th...   \n",
       "...                                                  ...   \n",
       "17453  [I wonder how many lives could’ve been saved i...   \n",
       "17454  [The @thetimes front page on 17th March. The f...   \n",
       "17455  [Trump just completed the racism trifecta in a...   \n",
       "17456  [Here are a few of my photographs from today’s...   \n",
       "17457  [‘IT’S GONE’: Bill De Blasio Says NYC Facing $...   \n",
       "\n",
       "                                                  source  \\\n",
       "0      According to the New York Times, Warner Bros. ...   \n",
       "1      Hurricane Hanna has made landfall in Texas.\\n\\...   \n",
       "2      Monkeys on the loose in India with stolen coro...   \n",
       "3                                                          \n",
       "4      “If Trump felt comfortable having it here, the...   \n",
       "...                                                  ...   \n",
       "17453  I wonder how many lives could’ve been saved if...   \n",
       "17454  The @thetimes front page on 17th March. The fi...   \n",
       "17455  Trump just completed the racism trifecta in a ...   \n",
       "17456  Here are a few of my photographs from today’s ...   \n",
       "17457  ‘IT’S GONE’: Bill De Blasio Says NYC Facing $9...   \n",
       "\n",
       "                                                 replies  \n",
       "0                                                         \n",
       "1      @TexasTribune Guess what the cause of death is...  \n",
       "2                                                         \n",
       "3      @BelAkinyii When was the last time you washed ...  \n",
       "4      @HeidiNBC These Trump fans have a right to att...  \n",
       "...                                                  ...  \n",
       "17453  @funder Wonder how many lives could have been ...  \n",
       "17454  @NadineDorries @thetimes Inadequate supplies o...  \n",
       "17455  @DNCWarRoom Fact check: Chinese is not a race....  \n",
       "17456  @Jess__Taylor__ @davidallengreen Eck! What are...  \n",
       "17457  @seanhannity DeBlasio caused NYC's lockdown ch...  \n",
       "\n",
       "[17458 rows x 6 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_json(\"./task2/data/processed_covid.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af053636",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
