# Rumour-Detection-and-Analysis-on-Twitter
### Task1 : Rumour Detection of Twitter
### Task2 : Rumour Analysis on Twitter

Name: Ari Boyd, Chenghao Li
Student Number: 992301, 1067999

For Task1, there's a kaggle competition involving 400+ students and we get a stable results that kept use #3 on the leaderboard.
#### Kaggle: 
https://www.kaggle.com/competitions/rumour-detection-and-analysis-on-twitter/leaderboard?

#### Report:
https://www.overleaf.com/read/bbxchdzbpvtv

## File Structure:

You will want to extract several compressed file to its current location, for example: extract /full_data/data_storage/full_dev_train.zip  to its current location  /full_data/data_storage/full_dev_train.json

### Task1: Rumour Classification Problems using BERT
./task1_BERT/*
BERT model used on Google Colab platform, with batch size tuning to 20, number of epochs to 15, and hidden dropout prob set to 0.1, using train+dev set to train, with truncated tweets used (yeah it outperforms using full text tweets) we get our best submission results.
./task1_BERT/bert_data/*
data used to train bert model

### Task2: Analysis rumour tweets structures
./task2_Analysis/Predict.ipynb Predict the label using the model (the prediction part is in fact done on Google Colab, this notebook just concatenate the result)
other files: Topic, Hashtag and sentiment analysis


### ./deprecated/* method tried before applying final BERT model

#### 1. ./tweetgetter:
./deprecated/tweetgetter/* Twitter APIs and Crawlers used through out the project, get twitter data by their IDs etc
./deprecated/Deprecated_Crawler/* Functional crawler but hasn't been used throughout the project
TwitterGetter.ipynb    # Scrap twitter data and Scrap twitter author data
TwitterGetter_V2.ipynb    # Scrap twitter data including author data for whole Train + Dev Set, however, this method WILL return text that may be truncated!!!
Analysis.ipynb    # analyse the twitter data

#### 2. Classic Models
./deprecated/models  previously tried models, including ClassicModel: MLP, SVC, LR and preprocessings such as PCA, TFIDF, bag of words etc. It also includes a deprecated BERT model which is trained on my GPU, however, the GPU memory restricted the training batch size, so later we move to GoogleColab cloud platform.
./deprecated/preprocessing_evalluations  bunch of relevant codes doing preprocessing and evaluations, trying to get original tweet data to "Source_Text" + List of "Reply_Text"

#### 3. ./data:
./data/* data needed to run the code, note that the path to these data need to be modified before any running and testing.
./id_data/     # stores Twitter IDs
./full_data/storage_data    # stores scrapped data , full data set for twitter and author
./full_data/*.json   # these file are generated by run-time program and can be changed so it's a temporary location

