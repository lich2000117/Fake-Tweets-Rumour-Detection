# Rumour-Detection-and-Analysis-on-Twitter

## Problem Address

As the Covid-19 pandemic unfolded, citizens and
organizations of many countries took to social
networking platforms to spread their knowledge
surrounding the phenomenon.

Some of this knowledge was based on substantiated
facts such as data from government sources like
the World Health Organization. However there was
also a large number of tweets making statements
that were based on hearsay, or personal opinion
presented as fact.

These statements, commonly referred to as
“rumours” are an important class of social media
objects to understand, as they could range
from harmless assertions, to potentially deadly
recommendations.

This project details processing and classifier training
on a set of ‘tweets’ from the popular online
platform ‘Twitter’. The project is divided into two part:

1. identifying tweets and
2. analyse rumours

## Data

### What we have

- a text file containing IDs of COVID-19 Related Tweets and IDs of their replies.
- a ground truth file containing IDs of Rumour and Non-Rumour Tweets.

### Preprocessing

- Group the IDs of replies and their source IDs.
- Get full tweets of provided IDs using Twitter API V1.1 and V2.
  
<img src="./mdplot/tweetobjori.png" width="900">

- Get data for their authors.

<img src="./mdplot/authorstat.png" width="900">

- Sort the replies based on their publish time.
- Gather their label (0: non-rumour, 1: rumour)
- Filter the dataset using two approches: NLP and Meta Data Analysis

<img src="./mdplot/nlpapproach.png" width="900">

### Approaches
Two approaches have been used in this project, purely based on linguistic features vs based on other attribute features.
1. NLP approach:
    Use words and sentences to do the predicition

2. Meta Data approach
    Use traditional attributes to do the prediction


## Potential Issues

1. Some tweets have been taken down / deleted so we can not get all data to work on.
2. Our prediction Model sizes around 1.5GB, so it can not be saved and have to be retrained.







### Task1 : Rumour Detection of Twitter
### Task2 : Rumour Analysis on Twitter


For Task1, there's a kaggle competition involving 400+ students and we get a stable results.
#### Kaggle: 
LeaderBoard Final Place: #3
Link: https://www.kaggle.com/competitions/rumour-detection-and-analysis-on-twitter/leaderboard?

#### Report:
Link: https://www.overleaf.com/read/bbxchdzbpvtv

## File Structure:

You will want to extract several compressed file to its current location, for example: extract /full_data/data_storage/full_dev_train.zip  to its current location  /full_data/data_storage/full_dev_train.json

### Task1: Rumour Classification Problems using BERT
./task1_BERT/*
BERT model used on Google Colab platform, with batch size tuning to 20, number of epochs to 15, and hidden dropout prob set to 0.1, using train+dev set to train, with truncated tweets used (yeah it outperforms using full text tweets) we get our best submission results.
./task1_BERT/bert_data/*
data used to train bert model

### Task2: Analysis rumour tweets structures
./task2_Analysis/Predict.ipynb Predict the label using the model (the prediction part is in fact done on Google Colab, this notebook just concatenate the result)
other files: Topic, Hashtag and sentiment analysis


### ./deprecated/* method tried before applying final BERT model

#### 1. ./tweetgetter:
./deprecated/tweetgetter/* Twitter APIs and Crawlers used through out the project, get twitter data by their IDs etc
./deprecated/Deprecated_Crawler/* Functional crawler but hasn't been used throughout the project
TwitterGetter.ipynb    # Scrap twitter data and Scrap twitter author data
TwitterGetter_V2.ipynb    # Scrap twitter data including author data for whole Train + Dev Set, however, this method WILL return text that may be truncated!!!
Analysis.ipynb    # analyse the twitter data

#### 2. Classic Models
./deprecated/models  previously tried models, including ClassicModel: MLP, SVC, LR and preprocessings such as PCA, TFIDF, bag of words etc. It also includes a deprecated BERT model which is trained on my GPU, however, the GPU memory restricted the training batch size, so later we move to GoogleColab cloud platform.
./deprecated/preprocessing_evalluations  bunch of relevant codes doing preprocessing and evaluations, trying to get original tweet data to "Source_Text" + List of "Reply_Text"

#### 3. ./data:
./data/* data needed to run the code, note that the path to these data need to be modified before any running and testing.
./id_data/     # stores Twitter IDs
./full_data/storage_data    # stores scrapped data , full data set for twitter and author
./full_data/*.json   # these file are generated by run-time program and can be changed so it's a temporary location

