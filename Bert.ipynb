{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82994e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import DefaultDataCollator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f770b5",
   "metadata": {},
   "source": [
    "## Load Data into Pandas DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca5906f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data\n",
    "\n",
    "TRAIN_DATA_PATH = \"./full_data/data_storage/full_train.json\"\n",
    "Y_TRAIN_ID_PATH = \"./id_data/train.label.txt\"\n",
    "X_TRAIN_ID_PATH = \"./id_data/train.data.txt\"   # train\n",
    "\n",
    "DEV_DATA_PATH = \"./full_data/data_storage/full_dev.json\"\n",
    "Y_DEV_ID_PATH = \"./id_data/dev.label.txt\"\n",
    "X_DEV_ID_PATH = \"./id_data/dev.data.txt\"   # dev\n",
    "\n",
    "TEST_DATA_PATH = \"./full_data/data_storage/test_full_v2.json\"\n",
    "X_TEST_ID_PATH = \"./id_data/test.data.txt\"   # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79bee2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_replies_id_to_sorted_text(list_of_id, data_df):\n",
    "    \"\"\"Convert a list of tweet ids to a list of its corresponding text in data_df\n",
    "    Sort by tweets original create time\"\"\"\n",
    "    if type(list_of_id) is not list:\n",
    "        # if input is a single id convert type\n",
    "        list_of_id = [list_of_id]\n",
    "    # Select dataframe based on a list of ids\n",
    "    selected_df = data_df[data_df['id'].astype(str).isin(list_of_id)]\n",
    "    # Sort by their created time\n",
    "    sorted_df = selected_df.sort_values(by = [\"created_at\"])\n",
    "    # Select text fields only\n",
    "    text_list = sorted_df[\"text\"].to_list()\n",
    "    return text_list\n",
    "\n",
    "\n",
    "def remove_URL(original):\n",
    "    \"\"\"Remove url link in the text\"\"\"\n",
    "    result = re.sub(r\"http\\S+\", \"\", original)\n",
    "    result = re.sub(r\"www.\\S+\", \"\", result)\n",
    "    result = re.sub(r\"wasap.my+\", \"\", result)\n",
    "    return result\n",
    "\n",
    "def join_data_id_label_v2(LABEL_PATH, ID_PATH, DATA_PATH):\n",
    "    # Process Labels\n",
    "    if LABEL_PATH:\n",
    "        # 1: Rumour\n",
    "        # 0: NonRumour\n",
    "        with open(LABEL_PATH, \"r\") as f:\n",
    "            y_label = f.read().strip().split(\"\\n\") # remove next line\n",
    "        y_label = pd.DataFrame(y_label, columns = [\"label\"])\n",
    "        y_label[y_label[\"label\"]==\"rumour\"] = 1\n",
    "        y_label[y_label[\"label\"]==\"nonrumour\"] = 0\n",
    "\n",
    "    ## Get Dataframe Id, with first id as source Id, and values as replies, not using dict since we have duplicated keys\n",
    "    total_id_list = []\n",
    "    with open(ID_PATH, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(',') # remove next line\n",
    "            source_id = line[0]\n",
    "            if len(line) > 1:\n",
    "                # if we have replies id\n",
    "                replies_id = line[1:]\n",
    "            else:\n",
    "                replies_id = []\n",
    "            row = [source_id, replies_id]\n",
    "            total_id_list.append(row)\n",
    "    len(total_id_list)\n",
    "    \n",
    "    ## Create a dataframe containing a list of replies\n",
    "    source_df = pd.DataFrame(total_id_list, columns = ['source_id', 'replies_id'])\n",
    "    data_df = pd.read_json(DATA_PATH)\n",
    "    source_df[\"reply_text_list\"] = source_df[\"replies_id\"].apply(convert_replies_id_to_sorted_text, data_df = data_df)\n",
    "    source_df[\"source_text\"] = source_df[\"source_id\"].apply(convert_replies_id_to_sorted_text, data_df = data_df)\n",
    "\n",
    "    if LABEL_PATH:\n",
    "      source_df[\"label\"] = y_label\n",
    "    return source_df\n",
    "\n",
    "def bert_preprocess(IDS, DATA, LABELS=False):\n",
    "    \"\"\"Function to combine all the preprocessing steps\"\"\"\n",
    "    data = join_data_id_label_v2(LABELS, IDS, DATA)\n",
    "    ## 1. Only keep english tweets as most of them are in english\n",
    "    # Use only text data and remove URLs\n",
    "    data[\"source\"] = data[\"source_text\"].apply(\" \".join).apply(remove_URL) \n",
    "    data[\"replies\"] = data[\"reply_text_list\"].apply(\" \".join).apply(remove_URL)\n",
    "    \n",
    "    \n",
    "    if LABELS:\n",
    "      data = data[['source','replies','label']]\n",
    "    else:\n",
    "      data = data[['source','replies']]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6da014f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>replies</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5. Can regularly rinsing your nose with saline...</td>\n",
       "      <td>4. Can eating garlic help prevent infection wi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>French police chief killed himself after #Char...</td>\n",
       "      <td>@Telegraph How very sad. @Telegraph @Telegraph...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus disease (COVID-19) advice for the ...</td>\n",
       "      <td>Infection control for suspected or confirmed C...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ottawa police confirm that there were multiple...</td>\n",
       "      <td>@WSJ Killers go berserk when cornered.  Hencef...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>if the primary focus of a government isn't to ...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1890</th>\n",
       "      <td>Desperate Ted Cruz Claims Planned Parenthood S...</td>\n",
       "      <td>@Bipartisanism \\nDesperate! @Bipartisanism  Cr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1891</th>\n",
       "      <td>\"Thoughts and prayers are not enough.\" Pres. O...</td>\n",
       "      <td>.@ABC has anyone else noticed mass shootings s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1892</th>\n",
       "      <td>Police have surrounded this building where the...</td>\n",
       "      <td>@NBCNews bury them in their hole @NBCNews @Wik...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1893</th>\n",
       "      <td></td>\n",
       "      <td>@Kirstenjoyweiss @MattFabrication @prestone85 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1894</th>\n",
       "      <td>@lynneSimpkin I can help! üë©‚Äçüè´\\n9am: Socialism ...</td>\n",
       "      <td>Excellent.  @rosierawle Tasha and I just littl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1895 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 source  \\\n",
       "0     5. Can regularly rinsing your nose with saline...   \n",
       "1     French police chief killed himself after #Char...   \n",
       "2     Coronavirus disease (COVID-19) advice for the ...   \n",
       "3     Ottawa police confirm that there were multiple...   \n",
       "4     if the primary focus of a government isn't to ...   \n",
       "...                                                 ...   \n",
       "1890  Desperate Ted Cruz Claims Planned Parenthood S...   \n",
       "1891  \"Thoughts and prayers are not enough.\" Pres. O...   \n",
       "1892  Police have surrounded this building where the...   \n",
       "1893                                                      \n",
       "1894  @lynneSimpkin I can help! üë©‚Äçüè´\\n9am: Socialism ...   \n",
       "\n",
       "                                                replies label  \n",
       "0     4. Can eating garlic help prevent infection wi...     0  \n",
       "1     @Telegraph How very sad. @Telegraph @Telegraph...     1  \n",
       "2     Infection control for suspected or confirmed C...     0  \n",
       "3     @WSJ Killers go berserk when cornered.  Hencef...     0  \n",
       "4                                                           0  \n",
       "...                                                 ...   ...  \n",
       "1890  @Bipartisanism \\nDesperate! @Bipartisanism  Cr...     1  \n",
       "1891  .@ABC has anyone else noticed mass shootings s...     1  \n",
       "1892  @NBCNews bury them in their hole @NBCNews @Wik...     0  \n",
       "1893  @Kirstenjoyweiss @MattFabrication @prestone85 ...     0  \n",
       "1894  Excellent.  @rosierawle Tasha and I just littl...     0  \n",
       "\n",
       "[1895 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Keep only source text as a column AND concatenated reply strings as another column\n",
    "data_train = bert_preprocess(X_TRAIN_ID_PATH, TRAIN_DATA_PATH, LABELS=Y_TRAIN_ID_PATH)\n",
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4689b12a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>replies</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COVID-19 Fact:\\nAre hand dryers effective in k...</td>\n",
       "      <td>@WeatherBug They are, in fact, germ-breeding f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@atruchecks when can we expect the result of m...</td>\n",
       "      <td>@ewart_lynne @atruchecks Hi have you had any l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does COVID-19 spread? \\n\\nPeople can catch...</td>\n",
       "      <td>I've read a lot about Corona virus lately and ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>every news outlet using headlines like,\\n\\n\"ar...</td>\n",
       "      <td>@TuckyAalto Apparently, when a headline is a q...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Researcher @naskrecki on his encounter with a ...</td>\n",
       "      <td>@Harvard @naskrecki eu tenho uma dessas em cas...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>or cure for COVID-19. However, there are sever...</td>\n",
       "      <td>WHAT ARE THE TREATMENT OPTIONS FOR COVID-19 (I...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>After speculation that he‚Äôs been arrested, Ban...</td>\n",
       "      <td>@artnet @xklamation there was a story saying s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>*Your questions answered*‚ùì\\n\\n*Reply with the ...</td>\n",
       "      <td>s?\\n\\n14. Can I catch COVID-19 from infected s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>‚ñ∫#Anonymous Operation #KKK ‚ñ∫Ku Klux Klan, We n...</td>\n",
       "      <td>@AnonymousVideo  @grannyrosie3 @AnonymousVideo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>Exposing yourself to the sun or to temperature...</td>\n",
       "      <td>Just as fast as the virus has spread so has th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>632 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                source  \\\n",
       "0    COVID-19 Fact:\\nAre hand dryers effective in k...   \n",
       "1    @atruchecks when can we expect the result of m...   \n",
       "2    How does COVID-19 spread? \\n\\nPeople can catch...   \n",
       "3    every news outlet using headlines like,\\n\\n\"ar...   \n",
       "4    Researcher @naskrecki on his encounter with a ...   \n",
       "..                                                 ...   \n",
       "627  or cure for COVID-19. However, there are sever...   \n",
       "628  After speculation that he‚Äôs been arrested, Ban...   \n",
       "629  *Your questions answered*‚ùì\\n\\n*Reply with the ...   \n",
       "630  ‚ñ∫#Anonymous Operation #KKK ‚ñ∫Ku Klux Klan, We n...   \n",
       "631  Exposing yourself to the sun or to temperature...   \n",
       "\n",
       "                                               replies label  \n",
       "0    @WeatherBug They are, in fact, germ-breeding f...     0  \n",
       "1    @ewart_lynne @atruchecks Hi have you had any l...     0  \n",
       "2    I've read a lot about Corona virus lately and ...     0  \n",
       "3    @TuckyAalto Apparently, when a headline is a q...     0  \n",
       "4    @Harvard @naskrecki eu tenho uma dessas em cas...     0  \n",
       "..                                                 ...   ...  \n",
       "627  WHAT ARE THE TREATMENT OPTIONS FOR COVID-19 (I...     0  \n",
       "628  @artnet @xklamation there was a story saying s...     1  \n",
       "629  s?\\n\\n14. Can I catch COVID-19 from infected s...     0  \n",
       "630  @AnonymousVideo  @grannyrosie3 @AnonymousVideo...     1  \n",
       "631  Just as fast as the virus has spread so has th...     0  \n",
       "\n",
       "[632 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Keep only source text as a column AND concatenated reply strings as another column\n",
    "data_dev = bert_preprocess(X_DEV_ID_PATH, DEV_DATA_PATH, LABELS=Y_DEV_ID_PATH)\n",
    "data_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defe64c0",
   "metadata": {},
   "source": [
    "## Convert Pandas Dataframe to Transformer DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "814c1f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source', 'replies', 'label'],\n",
       "    num_rows: 1895\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train = Dataset.from_pandas(data_train)\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54460f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source', 'replies', 'label'],\n",
       "    num_rows: 632\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dev = Dataset.from_pandas(data_dev)\n",
    "dataset_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2768c93",
   "metadata": {},
   "source": [
    "## Transformer Dataset to TensorFlow TF Dataset\n",
    "\n",
    "Makin use of Transformer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd99ff2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29.0/29.0 [00:00<00:00, 31.0kB/s]\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 570/570 [00:00<00:00, 390kB/s]\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 208k/208k [00:01<00:00, 180kB/s]\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 426k/426k [00:01<00:00, 326kB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  9.43ba/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 14.27ba/s]\n"
     ]
    }
   ],
   "source": [
    "## Set up Tokenizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "## Tokenize with two sentences separated by [SEP]m, use source and reply as two sentences\n",
    "def tokenize_function(dataset):\n",
    "    return tokenizer(dataset[\"source\"], dataset[\"replies\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Set up Train data\n",
    "tokenized_train_datasets = dataset_train.map(tokenize_function, batched=True)\n",
    "# Set up Dev Data\n",
    "tokenized_dev_datasets = dataset_dev.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27e65fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use data_collator to batch the dataset\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42b72ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 17:53:09.342915: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-06 17:53:09.343493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-06 17:53:09.343599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-06 17:53:09.343644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-06 17:53:09.757712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-06 17:53:09.757812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-06 17:53:09.757863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-06 17:53:09.758097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9531 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "tf_train_dataset = tokenized_train_datasets.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"label\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "tf_validation_dataset = tokenized_dev_datasets.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"label\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb0a0d4",
   "metadata": {},
   "source": [
    "## Tensor Flow Bert Model\n",
    "\n",
    "Use Training Set to train and test against dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe7868c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a7f7449",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 502M/502M [01:28<00:00, 5.95MB/s]\n",
      "2022-05-06 17:54:42.556141: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "## Define Model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n",
    "\n",
    "## Set up optimisation method, minimise which loss\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=tf.metrics.SparseCategoricalAccuracy(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "150f1ab8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "236/236 [==============================] - 70s 268ms/step - loss: 0.4196 - sparse_categorical_accuracy: 0.7892 - val_loss: 0.3116 - val_sparse_categorical_accuracy: 0.8639\n",
      "Epoch 2/3\n",
      "236/236 [==============================] - 63s 269ms/step - loss: 0.2341 - sparse_categorical_accuracy: 0.9115 - val_loss: 0.2069 - val_sparse_categorical_accuracy: 0.9225\n",
      "Epoch 3/3\n",
      "236/236 [==============================] - 63s 268ms/step - loss: 0.1137 - sparse_categorical_accuracy: 0.9650 - val_loss: 0.2446 - val_sparse_categorical_accuracy: 0.9304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8a31d7b5e0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model\n",
    "model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4b7906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44c54bb5",
   "metadata": {},
   "source": [
    "## Test on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "439c336e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = bert_preprocess(X_TEST_ID_PATH, TEST_DATA_PATH)\n",
    "test_dataset = Dataset.from_pandas(test_df)  # Convert to Transformer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bcae3ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 14.78ba/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert to Keras input for Bert model\n",
    "tokenized_test_datasets = test_dataset.map(tokenize_function, batched=True)\n",
    "tf_test_dataset = tokenized_test_datasets.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19319ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(tf_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "059b2767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the csv for prediction\n",
    "def generate_csv(pred, csv_name):\n",
    "    ids = pd.Index(range(len(pred)), name='Id')\n",
    "    predictions = pd.DataFrame(pred, index=ids)\n",
    "    predictions.columns = ['Predicted']\n",
    "    predictions.to_csv(csv_name)\n",
    "\n",
    "def model_output_to_label(model_output):\n",
    "    \"\"\"Conver the output class of a tensorflow model to label\"\"\"\n",
    "    logit_df = pd.DataFrame(model_output.to_tuple()[0], columns = [\"0\",\"1\"])\n",
    "    ## Choose highest logit as the predicted class\n",
    "    logit_df[\"label\"] = logit_df.apply(lambda x: 0 if x[\"0\"] > x[\"1\"] else 1, axis=1)\n",
    "    return logit_df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9d069dcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = model_output_to_label(y_pred)\n",
    "generate_csv(labels, \"./output/vanillabertmodel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9e5622",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
