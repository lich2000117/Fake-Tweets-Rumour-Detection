{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a79126c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe6ef0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_TWITTER_PATH = \"./full_data/data_storage/full_train_source_only.json\"\n",
    "FULL_TWITTER_PATH = \"./full_data/data_storage/full_train.json\"\n",
    "Y_TRAIN_PATH = \"./id_data/train.label.txt\"\n",
    "X_TRAIN_PATH = \"./id_data/train.data.txt\"   # train\n",
    "SOURCE_STORY_ONLY = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d7e6adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_data_id_label( LABEL_PATH, ID_PATH, DATA_PATH, dropna_on_column=\"text\", SOURCE_STORY_ONLY=True):\n",
    "    \"\"\"A function that joins data and id and their labels, return a dataframe that trims off twitter we don't have data\"\"\"\n",
    "    # Process Labels\n",
    "    # 1: Rumour\n",
    "    # 0: NonRumour\n",
    "    with open(LABEL_PATH, \"r\") as f:\n",
    "        y_label = f.read().strip().split(\"\\n\") # remove next line\n",
    "    y_label = pd.DataFrame(y_label, columns = [\"label\"])\n",
    "    y_label[y_label[\"label\"]==\"rumour\"] = 1\n",
    "    y_label[y_label[\"label\"]==\"nonrumour\"] = 0\n",
    "    \n",
    "    ## Get Dataframe Id\n",
    "    total_id_list = []\n",
    "    with open(ID_PATH, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip() # remove next line\n",
    "            if SOURCE_STORY_ONLY:\n",
    "                line = line.split(',')[0] # split into list\\\n",
    "            else:\n",
    "                line = line.split(',')\n",
    "            total_id_list.append(line)\n",
    "    if not SOURCE_STORY_ONLY:\n",
    "        total_id_list = [item for sublist in total_id_list for item in sublist] # Flat into a single list\n",
    "    total_id_list = total_id_list\n",
    "    id_df = pd.DataFrame(total_id_list, columns=[\"id\"])\n",
    "    id_df[\"id\"] = id_df[\"id\"].astype(str)\n",
    "    # join their labels\n",
    "    id_df[\"label\"] = y_label[\"label\"]\n",
    "    \n",
    "    ## Get Twitter Data\n",
    "    # Read Source Data\n",
    "    data_df = pd.read_json(DATA_PATH)\n",
    "    #source_df = pd.concat([source_df, y_train], axis=1)\n",
    "    data_df[\"id\"] = data_df[\"id\"].astype(str)\n",
    "    \n",
    "    ## Join the data file and IDs we got \n",
    "    ## Drop rows we don't have data on\n",
    "    combined_df = id_df.join(data_df.set_index('id'), on='id').dropna(subset=['text', 'author_id']).reset_index(drop=True)\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8a7b552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Remove_URL(original):\n",
    "    \"\"\"Remove url link in the text\"\"\"\n",
    "    result = re.sub(r\"http\\S+\", \"\", original)\n",
    "    result = re.sub(r\"www.\\S+\", \"\", result)\n",
    "    result = re.sub(r\"wasap.my+\", \"\", result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f8a9c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://stackoverflow.com/questions/27084617/detect-strings-with-non-english-characters-in-python\n",
    "def isEnglish(s):\n",
    "    \"\"\"Check if a string is english\"\"\"\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0257eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove Stopwords\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def text_processing(input_str):\n",
    "    \"\"\"A function takes a input text string and output a list of preprocessed words\n",
    "        1. Remove URL\n",
    "        2. StopWords Removal\n",
    "        3. Remove Non-English Words\n",
    "        4. Remove Numeric Value\n",
    "        5. Lemmatize\n",
    "    \n",
    "    \"\"\"\n",
    "    list_words = []\n",
    "    # Remove URL\n",
    "    input_str = Remove_URL(input_str)\n",
    "    for word in re.sub(r'[^\\w\\s]','', input_str).split():\n",
    "        # Remove Numeric Value, Non-English words and Stop words\n",
    "        if (word.isalpha()) and (isEnglish(word)) and (word not in stopwords.words('english')):\n",
    "            # Remove Links in the text\n",
    "            # Lemmatize words\n",
    "            word = lemmatizer.lemmatize(word)\n",
    "            # Add to List\n",
    "            list_words.append(word)\n",
    "    return list_words\n",
    "\n",
    "#filtered_words = [word for word in word_list if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2d3b443",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Expand list of words into bag-of-words array\n",
    "def get_bag_of_words(df_column):\n",
    "    \"\"\"Expand list of words into bag-of-words array, return the dataframe.\"\"\"\n",
    "    return pd.get_dummies(df_column.apply(pd.Series).stack()).groupby(level=0).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "263072ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "X_train = join_data_id_label(Y_TRAIN_PATH, X_TRAIN_PATH, SOURCE_TWITTER_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c3d3964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lang\n",
      "en      1547\n",
      "und        4\n",
      "es         2\n",
      "in         2\n",
      "ro         2\n",
      "hu         1\n",
      "it         1\n",
      "ja         1\n",
      "ru         1\n",
      "zh         1\n",
      "dtype: int64\n",
      "============\n",
      "withheld\n",
      "False       1551\n",
      "True          11\n",
      "dtype: int64\n",
      "============\n",
      "reply_settings\n",
      "everyone          1562\n",
      "dtype: int64\n",
      "============\n",
      "source                 \n",
      "Twitter Web App            406\n",
      "Twitter for Android        291\n",
      "Twitter for iPhone         286\n",
      "TweetDeck                  165\n",
      "Twitter Web Client         110\n",
      "                          ... \n",
      "The New York Times           1\n",
      "TheLatestIs                  1\n",
      "TweetCaster for Android      1\n",
      "Sendible                     1\n",
      "AOL Blogsmith                1\n",
      "Length: 61, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X_train[[\"lang\"]].value_counts())\n",
    "print(\"============\")\n",
    "print(X_train[[\"withheld\"]].value_counts())\n",
    "print(\"============\")\n",
    "print(X_train[[\"reply_settings\"]].value_counts())\n",
    "print(\"============\")\n",
    "print(X_train[[\"source\"]].value_counts())\n",
    "\n",
    "## Only keep English! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd208d55",
   "metadata": {},
   "source": [
    "###  Standard Scaler to do PCA to do dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab8e1e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c06ff02",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standardize, why? it needs to do to use PCA, also, it needs to have variance of 1\n",
    "scaler = StandardScaler()\n",
    "\n",
    "pca = PCA(n_components=100, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13886f6c",
   "metadata": {},
   "source": [
    "# Only Use Text Data\n",
    "other fields including author information, entities, public matrix, and source not used!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac5e495",
   "metadata": {},
   "source": [
    "### Process Training Data use above functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e639763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## How to normalize json field in the data? \n",
    "\n",
    "## NOT YET USED!\n",
    "#pd.json_normalize(X_train['public_metrics']).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d00646a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1250219300389974016</td>\n",
       "      <td>en</td>\n",
       "      <td>5. Can regularly rinsing your nose with saline...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Can, regularly, rinsing, nose, saline, help, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>554886875303780352</td>\n",
       "      <td>en</td>\n",
       "      <td>French police chief killed himself after #Char...</td>\n",
       "      <td>1</td>\n",
       "      <td>[French, police, chief, killed, CharlieHebdo, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1237901309011021825</td>\n",
       "      <td>en</td>\n",
       "      <td>Coronavirus disease (COVID-19) advice for the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Coronavirus, disease, advice, public, Should,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>524958128392376320</td>\n",
       "      <td>en</td>\n",
       "      <td>Ottawa police confirm that there were multiple...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Ottawa, police, confirm, multiple, suspect, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1239295488677085185</td>\n",
       "      <td>en</td>\n",
       "      <td>if the primary focus of a government isn't to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[primary, focus, government, isnt, alleviate, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id lang  \\\n",
       "0  1250219300389974016   en   \n",
       "1   554886875303780352   en   \n",
       "2  1237901309011021825   en   \n",
       "3   524958128392376320   en   \n",
       "4  1239295488677085185   en   \n",
       "\n",
       "                                                text label  \\\n",
       "0  5. Can regularly rinsing your nose with saline...     0   \n",
       "1  French police chief killed himself after #Char...     1   \n",
       "2  Coronavirus disease (COVID-19) advice for the ...     0   \n",
       "3  Ottawa police confirm that there were multiple...     0   \n",
       "4  if the primary focus of a government isn't to ...     0   \n",
       "\n",
       "                                          clean_text  \n",
       "0  [Can, regularly, rinsing, nose, saline, help, ...  \n",
       "1  [French, police, chief, killed, CharlieHebdo, ...  \n",
       "2  [Coronavirus, disease, advice, public, Should,...  \n",
       "3  [Ottawa, police, confirm, multiple, suspect, s...  \n",
       "4  [primary, focus, government, isnt, alleviate, ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "X_train = join_data_id_label(Y_TRAIN_PATH, X_TRAIN_PATH, SOURCE_TWITTER_PATH)\n",
    "## 1. Only keep english tweets as most of them are in english\n",
    "## 2. Based on the values count, drop irrelevant features \"reply_settings\" too\n",
    "X_train = X_train[X_train.lang==\"en\"].reset_index(drop=True)\n",
    "X_train = X_train[['id','lang', 'text','label']]\n",
    "# Use only text data and tokenize\n",
    "X_train[\"clean_text\"] = X_train[\"text\"].apply(text_processing)\n",
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1027e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>AB</th>\n",
       "      <th>ABC</th>\n",
       "      <th>ABCNews</th>\n",
       "      <th>ABOUT</th>\n",
       "      <th>AC</th>\n",
       "      <th>ACCORDING</th>\n",
       "      <th>ACCURATE</th>\n",
       "      <th>ACTED</th>\n",
       "      <th>AFFECT</th>\n",
       "      <th>...</th>\n",
       "      <th>youd</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>yourbabazg</th>\n",
       "      <th>youre</th>\n",
       "      <th>youve</th>\n",
       "      <th>zaelefty</th>\n",
       "      <th>zero</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zwinst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5681 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  AB  ABC  ABCNews  ABOUT  AC  ACCORDING  ACCURATE  ACTED  AFFECT  ...  \\\n",
       "0  0   0    0        0      0   0          0         0      0       0  ...   \n",
       "1  0   0    0        0      0   0          0         0      0       0  ...   \n",
       "2  0   0    0        0      0   0          0         0      0       0  ...   \n",
       "3  0   0    0        0      0   0          0         0      0       0  ...   \n",
       "4  0   0    0        0      0   0          0         0      0       0  ...   \n",
       "\n",
       "   youd  young  younger  yourbabazg  youre  youve  zaelefty  zero  zoom  \\\n",
       "0     0      0        0           0      0      0         0     0     0   \n",
       "1     0      0        0           0      0      0         0     0     0   \n",
       "2     0      0        0           0      0      0         0     0     0   \n",
       "3     0      0        0           0      0      0         0     0     0   \n",
       "4     0      0        0           0      0      0         0     0     0   \n",
       "\n",
       "   zwinst  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  \n",
       "\n",
       "[5 rows x 5681 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "X_train = join_data_id_label(Y_TRAIN_PATH, X_TRAIN_PATH, SOURCE_TWITTER_PATH)\n",
    "\n",
    "\n",
    "## 1. Only keep english tweets as most of them are in english\n",
    "## 2. Based on the values count, drop irrelevant features \"reply_settings\" too\n",
    "X_train = X_train[X_train.lang==\"en\"].reset_index(drop=True)\n",
    "X_train = X_train[['id','lang', 'text','label']]\n",
    "# Use only text data and tokenize\n",
    "X_train[\"clean_text\"] = X_train[\"text\"].apply(text_processing)\n",
    "# Get BOF dataframe\n",
    "X_train_BOW = get_bag_of_words(X_train[\"clean_text\"])\n",
    "X_train_BOW.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5d14815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train test data\n",
    "y_train = X_train[\"label\"]\n",
    "X_train = X_train_BOW\n",
    "\n",
    "assert(len(X_train) == len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe0a1d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform using Train Data\n",
    "\n",
    "train_columns = X_train.columns\n",
    "\n",
    "# StandardScaler\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=train_columns)\n",
    "\n",
    "# PCA\n",
    "X_train_scaled_reduced = pd.DataFrame(pca.fit_transform(X_train_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9605e4e",
   "metadata": {},
   "source": [
    "### Dev Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffbd0ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_training_columns_only(test_BOW_df, train_column, default_value=0):\n",
    "    \"\"\"Take a BOW dataframe and keep only the words from training dataframe, return a dataframe\"\"\"\n",
    "    return test_BOW_df.reindex(columns = train_columns, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c44736cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV_SOURCE_TWITTER_PATH = \"./full_data/data_storage/full_dev_source_only.json\"\n",
    "#DEV_FULL_TWITTER_PATH = \"./full_data/data_storage/full_dev.json\"\n",
    "Y_DEV_PATH = \"./id_data/dev.label.txt\"\n",
    "X_DEV_PATH = \"./id_data/dev.data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41cf5839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>AB</th>\n",
       "      <th>ABC</th>\n",
       "      <th>ABCNews</th>\n",
       "      <th>ABOUT</th>\n",
       "      <th>AC</th>\n",
       "      <th>ACCORDING</th>\n",
       "      <th>ACCURATE</th>\n",
       "      <th>ACTED</th>\n",
       "      <th>AFFECT</th>\n",
       "      <th>...</th>\n",
       "      <th>youd</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>yourbabazg</th>\n",
       "      <th>youre</th>\n",
       "      <th>youve</th>\n",
       "      <th>zaelefty</th>\n",
       "      <th>zero</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zwinst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5681 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  AB  ABC  ABCNews  ABOUT  AC  ACCORDING  ACCURATE  ACTED  AFFECT  ...  \\\n",
       "0  0   0    0        0      0   0          0         0      0       0  ...   \n",
       "1  0   0    0        0      0   0          0         0      0       0  ...   \n",
       "2  0   0    0        0      0   0          0         0      0       0  ...   \n",
       "3  0   0    0        0      0   0          0         0      0       0  ...   \n",
       "4  0   0    0        0      0   0          0         0      0       0  ...   \n",
       "\n",
       "   youd  young  younger  yourbabazg  youre  youve  zaelefty  zero  zoom  \\\n",
       "0     0      0        0           0      0      0         0     0     0   \n",
       "1     0      0        0           0      0      0         0     0     0   \n",
       "2     0      0        0           0      0      0         0     0     0   \n",
       "3     0      0        0           0      0      0         0     0     0   \n",
       "4     0      0        0           0      0      0         0     0     0   \n",
       "\n",
       "   zwinst  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  \n",
       "\n",
       "[5 rows x 5681 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "X_dev = join_data_id_label(Y_DEV_PATH, X_DEV_PATH, DEV_SOURCE_TWITTER_PATH)\n",
    "\n",
    "\n",
    "## 1. Only keep english tweets as most of them are in english\n",
    "## 2. Based on the values count, drop irrelevant features \"reply_settings\" too\n",
    "X_dev = X_dev[X_dev.lang==\"en\"].reset_index(drop=True)\n",
    "X_dev = X_dev[['id','lang', 'text','label']]\n",
    "# Use only text data and tokenize\n",
    "X_dev[\"clean_text\"] = X_dev[\"text\"].apply(text_processing)\n",
    "# Get BOF dataframe\n",
    "X_dev_BOW = get_bag_of_words(X_dev[\"clean_text\"])\n",
    "\n",
    "## Keep training columns only\n",
    "X_dev_BOW = keep_training_columns_only(X_dev_BOW, train_columns, default_value=0)\n",
    "\n",
    "X_dev_BOW.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c51e3fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train test data\n",
    "y_dev = X_dev[\"label\"]\n",
    "X_dev = X_dev_BOW\n",
    "\n",
    "assert(len(X_dev) == len(y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33c5bac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform using Trained Transformer\n",
    "\n",
    "# StandardScaler\n",
    "X_dev_scaled = pd.DataFrame(scaler.transform(X_dev), columns=train_columns)\n",
    "# PCA\n",
    "X_dev_scaled_reduced = pd.DataFrame(pca.transform(X_dev_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a45f66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e8c9ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
